# -*- coding: utf-8 -*-
"""22082809_Coursework_Programming_Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Bnqu44499n_kP1RQGw618Ot-NweTGAW-

# **Loading the Datasets**
"""

import pandas as pd

# Read the CSV file
df = pd.read_csv('/content/Marketing Campaign data.csv')

print(df.head())

"""# **Meta Data Table for Data Understanding**"""

import pandas as pd
from tabulate import tabulate

# Read the CSV file into a pandas DataFrame
df = pd.read_csv('/content/Marketing Campaign data.csv')

# Function to limit the display of unique values
shortened_unique_values = lambda lst, max_disp=3, max_len=20: (", ".join(map(str, lst[:max_disp]))
+ (", ..." if len(lst) > max_disp else "")
if len(", ".join(map(str, lst[:max_disp]))) <= max_len else ", ".join(map(str, lst[:max_disp]))[:max_len - 3] + "...")

# Function to limit the length of the mode
truncated_mode = lambda mode_str, max_len=15: (mode_str[:max_len - 3]
                                               + "..." if mode_str and len(mode_str) > max_len else mode_str)

# Create metadata
metadata = []
for column in df.columns:
    data_type = df[column].dtype
    unique_values = shortened_unique_values(df[column].unique())
    max_value = df[column].max() if data_type in ['int64', 'float64'] else None
    min_value = df[column].min() if data_type in ['int64', 'float64'] else None
    mean = df[column].mean() if data_type in ['int64', 'float64'] else None
    std_dev = df[column].std() if data_type in ['int64', 'float64'] else None
    mode = truncated_mode(str(df[column].mode().values[0])) if len(df[column].mode()) > 0 else None
    histogram = True if data_type in ['int64', 'float64'] else False
    bar_chart = True if data_type not in ['int64', 'float64'] else False

    metadata.append((column, data_type, unique_values, max_value, min_value, mean, std_dev, mode, histogram, bar_chart))

# Create metadata DataFrame
metadata_df = pd.DataFrame(metadata, columns=['Variable Name', 'Data Type', 'Unique Values', 'Maximum', 'Minimum', 'Mean', 'Std. Deviation', 'Mode', 'Histogram', 'Bar Chart'])

# Save metadata to CSV
metadata_df.to_csv('metadata_table.csv', index=False)
print("Metadata table has been saved to metadata_table.csv")

# Display the metadata in a tabular format
metadata_table = tabulate(metadata_df, headers='keys', tablefmt='grid', showindex=False)
print("Metadata table for all attributes:")
print(metadata_table)

"""# **Histograms and Bar Charts for respective variables**"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import math

# Shorten x-axis labels
shorten_label = lambda label, max_len=5: label[:max_len - 1] + '...' if len(label) > max_len else label

# Initialize lists for histograms and bar charts
has_hist, has_bar = [], []

# Identify columns for histograms and bar charts
for col in df.columns:
    (has_hist if df[col].dtype in ['int64', 'float64'] else has_bar).append(col)

# Plot histograms
plt.figure(figsize=(15, 10))
for i, col in enumerate(has_hist):
    plt.subplot(math.ceil(len(has_hist) / 4), 4, i+1)
    sns.histplot(df[col], bins=10, kde=True)
    plt.title(f'Histogram of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')
plt.tight_layout()
plt.show()

# Plot bar charts
plt.figure(figsize=(15, 10))
for i, col in enumerate(has_bar):
    plt.subplot(math.ceil(len(has_bar) / 4), 4, i+1)
    shortened_labels = [shorten_label(label) for label in df[col].value_counts().index]
    df[col].value_counts().rename(index=dict(zip(df[col].value_counts().index, shortened_labels))).plot(kind='bar')
    plt.title(f'Bar Chart of {col}')
    plt.xlabel('Category')
    plt.ylabel('Count')
plt.tight_layout()
plt.show()

"""# **Finding the Unique Values, Missing Values and Error in Datasets**"""

import pandas as pd

# Initialize lists to store results
columns = df.columns.tolist()
unique_values = [df[column].nunique() for column in columns]
total_missing_values = [(df[column].isnull().sum() + (df[column].astype(str).str.strip() == '').sum() +
                         df[column].astype(str).str.lower().str.strip().eq('unknown').sum()) for column in columns]
error_values = [(df[column] == "#VALUE!").sum() if df[column].dtype == 'object' else 0 for column in columns]

# Create a DataFrame to store the results
results_df = pd.DataFrame({
    'Column': columns,
    'Unique Values': unique_values,
    'Total Missing Values': total_missing_values,
    'Error Values': error_values,
})

# Display the results DataFrame
print(results_df)

"""# **Remove Comments Columns as well as variable with ZERO influences**


"""

import pandas as pd

# Load your dataset
df = pd.read_csv('/content/Marketing Campaign data.csv')

# Check the unique values of the PRINTER_SUPPLIES column
print(df['PRINTER_SUPPLIES'].unique())

# Remove the PRINTER_SUPPLIES and COMMENTS columns
df = df.drop(['PRINTER_SUPPLIES', 'COMMENTS'], axis=1)

# Save the modified dataset
df.to_csv('modified_dataset.csv', index=False)

"""# **Finding Correlations with Target Variable "Affinity Card" and remove Comments Columns as well as variable with ZERO influences**"""

from sklearn.preprocessing import OrdinalEncoder

# Read the CSV file
df = pd.read_csv('/content/Marketing Campaign data.csv')

# Drop 'COMMENTS' column if it's not needed
df = df.drop(columns=['COMMENTS'])

# Apply Ordinal Encoding to categorical columns
ordinal_encoder = OrdinalEncoder()
df_encoded = df.copy()
df_encoded[df.select_dtypes(include=['object']).columns] = ordinal_encoder.fit_transform(df.select_dtypes(include=['object']))

# Calculate correlations with 'AFFINITY_CARD'
correlation = df_encoded.corr()['AFFINITY_CARD']

# Identify low-influential variables
low_influential = [key for key, value in correlation.items() if abs(value) == 0 or pd.isnull(value)]

# Remove low-influential variables from the DataFrame
df_filtered = df_encoded.drop(columns=low_influential)

# Display correlations with 'AFFINITY_CARD'
print("Correlation with ordinal encoding:")
print(correlation)

print("\nLow influential variables:")
print(low_influential)

# Display the names of the columns in the filtered DataFrame
print("\nFiltered DataFrame columns:")
print(df_filtered.columns)

print(df.shape)

"""# **Clean the records (remove records with missing values or errors if itâ€™s less than 5%))**

1.   Remove Printer Supplies column and COMMENTS columns

2. Now Our DataFrame is 1500 X 17    


"""

# Calculate the percentage of missing or error data in each column
for column in df.columns:
    # Count missing values
    num_missing = df[column].isnull().sum()

    # Count error values (e.g., '#VALUE!')
    num_error = (df[column] == '#VALUE!').sum()

    # Calculate total number of records
    total_records = len(df)

    # Calculate the percentage of missing or error values
    percent_missing = (num_missing / total_records) * 100
    percent_error = (num_error / total_records) * 100

    # Remove records with missing values or errors if the percentage is less than 5%
    if percent_missing < 5:
        df = df.dropna(subset=[column])
    if percent_error < 5:
        df = df[df[column] != '#VALUE!']

# Print the cleaned DataFrame
print("Data frame after Cleaning missing values and error values from records")
print(df.shape)

"""# **Transforming some variables using pandas library and basic python code**"""

import pandas as pd

# Read the data into a pandas DataFrame
#df = pd.read_csv('/content/Marketing Campaign data.csv')

# a) Transforming CUST_GENDER into binary (F - 0, M - 1)
# Basic Python code
df_basic_gender = df.copy()
df_basic_gender['CUST_GENDER'] = df_basic_gender['CUST_GENDER'].map({'F': 0, 'M': 1})

# Pandas library method
df_pandas_gender = df.copy()
df_pandas_gender['CUST_GENDER'] = pd.factorize(df_pandas_gender['CUST_GENDER'])[0]

# Now, update the original DataFrame with the transformed columns using the pandas method for consistency
df.update(df_basic_gender[['CUST_GENDER']])

print(df.head())

# b) Transforming COUNTRY_NAME into ordinal numbers based on occurrence
# Basic Python code
country_counts = df['COUNTRY_NAME'].value_counts()
country_dict = {country: idx for idx, country in enumerate(country_counts.index)}
df_basic_country = df.copy()
df_basic_country['COUNTRY_NAME'] = df_basic_country['COUNTRY_NAME'].map(country_dict)

# Pandas library method
df_pandas_country = df.copy()
df_pandas_country['COUNTRY_NAME'] = df_pandas_country['COUNTRY_NAME'].astype('category').cat.codes

# Now, update the original DataFrame with the transformed columns using the pandas method for consistency
df.update(df_basic_country[['COUNTRY_NAME']])

print(df.head())

# c) Transforming CUST_INCOME_LEVEL into 3 ordinal levels (1: low, 2: middle, 3: high)
# Basic Python code
income_levels ={'J: 190,000 - 249,999': 3,'I: 170,000 - 189,999': 3, 'H: 150,000 - 169,999': 3,
 'B: 30,000 - 49,999': 1, 'K: 250,000 - 299,999': 3, 'L: 300,000 and above': 3,
 'G: 130,000 - 149,999': 3, 'C: 50,000 - 69,999': 1, 'E: 90,000 - 109,999': 2,
 'D: 70,000 - 89,999': 2, 'F: 110,000 - 129,999': 2, 'A: Below 30,000': 1}
df_basic_income = df.copy()
df_basic_income['CUST_INCOME_LEVEL'] = df_basic_income['CUST_INCOME_LEVEL'].map(income_levels)

# Pandas library method
df_pandas_income = df.copy()
df_pandas_income['CUST_INCOME_LEVEL'] = pd.factorize(df_pandas_income['CUST_INCOME_LEVEL'], sort=True)[0] + 1

# Now, update the original DataFrame with the transformed columns using the pandas method for consistency
df.update(df_basic_income[['CUST_INCOME_LEVEL']])

print(df.head())

# d) Transforming EDUCATION into ordinal numbers based on USA education levels
# Basic Python code
education_levels = {'Presch.': 0, '1st-4th': 1, '5th-6th': 2, '7th-8th': 3, '9th': 4, '10th': 5, '11th': 6, '12th': 7,
                    'HS-grad': 8, 'Assoc-A': 9, 'Assoc-V': 10, 'Profsc': 11, 'Bach.': 12, 'Masters': 13, 'PhD': 14, '< Bach.': 15}
df_basic_education = df.copy()
df_basic_education['EDUCATION'] = df_basic_education['EDUCATION'].map(education_levels)

# Pandas library method
df_pandas_education = df.copy()
df_pandas_education['EDUCATION'] = pd.Categorical(
    df_pandas_education['EDUCATION'], categories=education_levels.keys(), ordered=True).codes

# Now, update the original DataFrame with the transformed columns using the pandas method for consistency
df.update(df_basic_education[['EDUCATION']])

print(df.head())

# e) Transforming HOUSEHOLD_SIZE into ordinal numbers based on the number of people
# Basic Python code
household_levels = {'1': 0, '2': 1, '3': 2, '4-5': 3, '6-8': 4, '9+': 5}
df_basic_household = df.copy()
df_basic_household['HOUSEHOLD_SIZE'] = df_basic_household['HOUSEHOLD_SIZE'].map(household_levels)

# Pandas library method
df_pandas_household = df.copy()
df_pandas_household['HOUSEHOLD_SIZE'] = pd.Categorical(
    df_pandas_household['HOUSEHOLD_SIZE'], categories=household_levels.keys(), ordered=True).codes

# Now, update the original DataFrame with the transformed columns using the pandas method for consistency
df.update(df_basic_household[['HOUSEHOLD_SIZE']])

print(df.head())

# Save the updated DataFrame to a new CSV file
df.to_csv('/content/Updated_Marketing_Campaign_data.csv', index=False)

print("Updated DataFrame has been saved to Updated_Marketing_Campaign_data.csv")

print(df.head)

"""# **Summary statistics of all variables for Data Analysis Without change the String value columns**"""

import pandas as pd
import numpy as np
from scipy.stats import skew, kurtosis

# Summary statistics for all columns
summary_stats_all = df.describe(include='all').transpose()

# Additional statistics: skewness and kurtosis for numeric columns
numeric_cols = df.select_dtypes(include=[np.number])
skewness = numeric_cols.apply(lambda x: skew(x.dropna()))
kurt = numeric_cols.apply(lambda x: kurtosis(x.dropna()))

# Fill NaN for numeric columns without skewness or kurtosis
skewness = skewness.fillna(np.nan)
kurt = kurt.fillna(np.nan)

# Concatenate the additional statistics to the summary statistics DataFrame for numeric columns
summary_stats_all['skewness'] = skewness
summary_stats_all['kurtosis'] = kurt

# Display the summary statistics for all columns
#print(summary_stats_all)

# Print summary statistics for all columns in tabular form, excluding top and unique values
print(summary_stats_all.to_string(index=True, columns=['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max', 'skewness', 'kurtosis']))

"""# **Summary statistics of all variables for Data Analysis With change the String value columns using Ordinal encoder**"""

from scipy.stats import skew, kurtosis
from sklearn.preprocessing import OrdinalEncoder
# Automatically find all string columns
string_columns = df.select_dtypes(include=[object]).columns

print("String Columns:")
print(string_columns)

# Ordinal encoding for string columns
encoder = OrdinalEncoder()
df_ordinal = df.copy()
df_ordinal[string_columns] = encoder.fit_transform(df[string_columns])

# Calculate summary statistics for all columns, including skewness and kurtosis
summary_stats_all = df_ordinal.describe(include='all').transpose()

# Skewness and kurtosis for numeric columns
numeric_cols = df_ordinal.select_dtypes(include=[np.number])
skewness = numeric_cols.apply(lambda x: skew(x.dropna()))
kurt = numeric_cols.apply(lambda x: kurtosis(x.dropna()))

# Add skewness and kurtosis to the summary statistics
summary_stats_all['skewness'] = skewness
summary_stats_all['kurtosis'] = kurt

# Summary statistics for all columns excluding 'top' and 'unique' values
summary_columns = ['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max', 'skewness', 'kurtosis']
print("Summary Statistics:")
print(summary_stats_all.to_string(index=True, columns=summary_columns))

"""# **Histogram plot of a variable which allow a user to choose in runtime. (Data Exploration)**"""

import pandas as pd
import matplotlib.pyplot as plt
# Function to show a histogram plot of a given column
def show_histogram(column_name):
    plt.figure()
    df[column_name].hist(bins=10, edgecolor='black')  # Adjust the number of bins as needed
    plt.title(f'Histogram of {column_name}')
    plt.xlabel(column_name)
    plt.ylabel('Frequency')
    plt.show()
# Main loop to interact with the user
while True:
    # List available numeric columns
    numeric_columns = df.select_dtypes(include=['number']).columns.tolist()

    # Display available numeric columns to the user
    print("\nAvailable columns for histogram:")
    for idx, col in enumerate(numeric_columns, 1):
        print(f"{idx}. {col}")

    # Ask user to choose a column
    user_input = input("\nChoose a column number to plot a histogram (or 'exit' to quit): ")

    if user_input.lower() == 'exit':
        break  # Exit the loop and end the program

    # Validate user input
    try:
        column_index = int(user_input) - 1
        if column_index < 0 or column_index >= len(numeric_columns):
            raise ValueError("Invalid index")
        chosen_column = numeric_columns[column_index]
        # Show the histogram plot for the chosen column
        show_histogram(chosen_column)
    except ValueError:
        print("Invalid input. Please enter a valid column number or 'exit' to quit.")

"""# **Build Logistics Regression Model for 1000 Random Customers (Data Mining)**"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler
from sklearn.utils import resample

# Step 1: Select 1000 random records from the DataFrame
random_sample = df.sample(n=1000)

# Step 2: Define the feature set (X) and the target variable (y)
# 'AFFINITY_CARD' is the target variable indicating whether a customer has an affinity card
target_variable = 'AFFINITY_CARD'
X = random_sample.drop(target_variable, axis=1)  # Features (excluding the target variable)
y = random_sample[target_variable]  # Target variable

# Step 3: Convert categorical features to numeric using dummy variables
X = pd.get_dummies(X, drop_first=True)  # One-hot encoding for categorical variables, dropping the first category to avoid multicollinearity

# Step 4: Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 5: Standardize the features to ensure they are on the same scale
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Step 6: Build a logistic regression model
logistic_regression_model = LogisticRegression(solver='liblinear', random_state=100)
logistic_regression_model.fit(X_train_scaled, y_train)  # Fit the model to the training data

# Step 7: Make predictions on the test set
y_pred = logistic_regression_model.predict(X_test_scaled)

# Step 8: Evaluate the model's performance
accuracy = accuracy_score(y_test, y_pred)
confusion = confusion_matrix(y_test, y_pred)
classification = classification_report(y_test, y_pred)
print("Accuracy:", accuracy)
print("Confusion Matrix:")
print(confusion)
print("Classification Report:")
print(classification)

# Step 9: Save the remaining data that was not used in the analysis
# Determine which records were not in the random sample
remaining_data = df.drop(random_sample.index)  # Drop the sampled records from the original data
remaining_data.to_csv("remaining_data.csv", index=False)  # Save to CSV

print("Remaining data saved to 'remaining_data.csv'.")

"""# **Build Prediction Application based on Logistic Regression Model**"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import OrdinalEncoder
import ipywidgets as widgets
from IPython.display import display
import io

# Load the dataset and preprocess it
data = pd.read_csv("/content/Marketing Campaign data.csv")  # Update with your file path
data = data.sample(frac=1).reset_index(drop=True)  # Shuffle the dataset
data = data[['AGE', 'CUST_GENDER', 'CUST_MARITAL_STATUS', 'CUST_INCOME_LEVEL', 'COUNTRY_NAME', 'EDUCATION', 'OCCUPATION', 'HOUSEHOLD_SIZE', 'YRS_RESIDENCE',
             'BULK_PACK_DISKETTES', 'FLAT_PANEL_MONITOR', 'HOME_THEATER_PACKAGE', 'BOOKKEEPING_APPLICATION', 'Y_BOX_GAMES', 'OS_DOC_SET_KANJI', 'AFFINITY_CARD']]

# Encoding dictionaries for custom variables

gender_levels = {'F': 0, 'M': 1}  # Updated to match input data
education_levels = {'Presch.': 0, '1st-4th': 1, '5th-6th': 2, '7th-8th': 3, '9th': 4, '10th': 5, '11th': 6, '12th': 7,
                    'HS-grad': 8, 'Assoc-A': 13, 'Assoc-V': 14, 'Profsc': 15, 'Bach.': 10, 'Masters': 11, 'PhD': 12, '< Bach.': 9}
household_levels = {'1': 0, '2': 1, '3': 2, '4-5': 3, '6-8': 4, '9+': 5}
income_levels = {'J: 190,000 - 249,999': 3, 'I: 170,000 - 189,999': 3, 'H: 150,000 - 169,999': 3,
                 'B: 30,000 - 49,999': 1, 'K: 250,000 - 299,999': 3, 'L: 300,000 and above': 3,
                 'G: 130,000 - 149,999': 3, 'C: 50,000 - 69,999': 1, 'E: 90,000 - 109,999': 2,
                 'D: 70,000 - 89,999': 2, 'F: 110,000 - 129,999': 2, 'A: Below 30,000': 1}
country_names = {'United States of America': 1, 'Brazil': 2, 'Argentina': 3, 'Germany': 4, 'Italy': 5, 'New Zealand': 6, 'Australia': 7,
                 'Poland': 8, 'Saudi Arabia': 9, 'Denmark': 10, 'Japan': 11, 'China': 12, 'Canada': 13, 'United Kingdom': 14, 'Singapore': 15,
                 'South Africa': 16, 'France': 17, 'Turkey': 18, 'Spain': 19}
marital_statuses = {'NeverM': 1, 'Married': 2, 'Divorc.': 3, 'Mabsent': 4, 'Separ.': 5, 'Widowed': 6, 'Mar-AF': 7}
occupations = {'Prof.': 1, 'Sales': 2, 'Cleric.': 3, 'Exec.': 4, 'Other': 5, 'Farming': 6, 'Transp.': 7, 'Machine': 8, 'Crafts': 9, 'Handler': 10,
               'Protec.': 11, 'TechSup': 12, 'House-s': 13, 'Armed-F': 14}

# Define entry widgets for each variable
widgets_list = []
for column in data.columns[:-1]:  # Exclude 'CUST_ID'
    if column == 'AGE':
        widget = widgets.FloatSlider(min=18, max=100, step=1, value=30, description='Age:')
    elif column == 'CUST_GENDER':
        widget = widgets.Dropdown(options=list(gender_levels.keys()), value=list(gender_levels.keys())[0], description='Gender:')
    elif column == 'CUST_MARITAL_STATUS':
        widget = widgets.Dropdown(options=list(marital_statuses.keys()), value=list(marital_statuses.keys())[0], description='Marital Status:')
    elif column == 'CUST_INCOME_LEVEL':
        widget = widgets.Dropdown(options=list(income_levels.keys()), value=list(income_levels.keys())[0], description='Income Level:')
    elif column == 'EDUCATION':
        widget = widgets.Dropdown(options=list(education_levels.keys()), value=list(education_levels.keys())[0], description='Education:')
    elif column == 'OCCUPATION':
        widget = widgets.Dropdown(options=list(occupations.keys()), value=list(occupations.keys())[0], description='Occupation:')
    elif column == 'HOUSEHOLD_SIZE':
        widget = widgets.FloatSlider(min=min(household_levels.values()), max=max(household_levels.values()), step=1,
                                      value=round(sum(household_levels.values()) / len(household_levels)), description='Household Size:')
    elif column == 'YRS_RESIDENCE':
        widget = widgets.FloatSlider(min=0, max=14, step=1, value=7, description='Years of Residence:')
    elif column in ['BULK_PACK_DISKETTES', 'FLAT_PANEL_MONITOR', 'HOME_THEATER_PACKAGE', 'BOOKKEEPING_APPLICATION', 'Y_BOX_GAMES', 'OS_DOC_SET_KANJI']:
        widget = widgets.Dropdown(options=['0', '1'], value='0', description=column.replace('_', ' ').title())
    elif column == 'COUNTRY_NAME':
        widget = widgets.Dropdown(options=list(country_names.keys()), value=list(country_names.keys())[0], description='Country Name:')
    else:
        widget = None
    if widget:
        widgets_list.append(widget)

# Split the dataset into training and testing sets
train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)
X_train = train_data.drop('AFFINITY_CARD', axis=1)
y_train = train_data['AFFINITY_CARD']
X_test = test_data.drop('AFFINITY_CARD', axis=1)
y_test = test_data['AFFINITY_CARD']

# Apply ordinal encoding to categorical features using combined dataset
combined_data = pd.concat([X_train, X_test], axis=0)
encoder = OrdinalEncoder()
combined_data_encoded = pd.DataFrame(encoder.fit_transform(combined_data), columns=combined_data.columns)

# Split the combined dataset back into training and testing sets
X_train_encoded = combined_data_encoded[:len(X_train)]
X_test_encoded = combined_data_encoded[len(X_train):]

# Train the logistic regression model
model = LogisticRegression()
model.fit(X_train_encoded, y_train)

def predict(input_data):
    DEFAULT_VALUE = 0
    input_data_encoded = []
    for i, value in enumerate(input_data):
        if isinstance(value, str):
            if i == 0:  # AGE
                input_data_encoded.append(float(value))
            elif i == 1:  # CUST_GENDER
                input_data_encoded.append(gender_levels[value])
            elif i == 2:  # CUST_MARITAL_STATUS
                input_data_encoded.append(marital_statuses[value])
            elif i == 3:  # CUST_INCOME_LEVEL
                input_data_encoded.append(income_levels[value])
            elif i == 4:  # COUNTRY_NAME
                input_data_encoded.append(country_names[value])
            elif i == 5:  # EDUCATION
                input_data_encoded.append(education_levels[value])
            elif i == 6:  # OCCUPATION
                input_data_encoded.append(occupations[value])
            elif i == 9 or i == 10 or i == 11 or i == 12 or i == 13 or i == 14:  # BULK_PACK_DISKETTES, FLAT_PANEL_MONITOR, HOME_THEATER_PACKAGE, BOOKKEEPING_APPLICATION, Y_BOX_GAMES, OS_DOC_SET_KANJI
                input_data_encoded.append(float(value))
            else:
                input_data_encoded.append(DEFAULT_VALUE)
        else:
            input_data_encoded.append(float(value))

    input_data_encoded = [input_data_encoded]
    input_data_encoded = np.array(input_data_encoded, dtype=np.float64)

    prediction = model.predict(input_data_encoded)
    probability = model.predict_proba(input_data_encoded)

    return prediction[0], probability[0]

def predict_uploaded_data(uploaded_data):
    # Drop any extra columns not present in the original dataset, ignoring errors
    uploaded_data = uploaded_data.drop(['CUST_ID','COMMENTS', 'AFFINITY_CARD', 'PRINTER_SUPPLIES'], axis=1, errors='ignore')

    # Take random 100 records from the uploaded data
    uploaded_data_sample = uploaded_data.sample(n=100, random_state=42)

    # Reorder columns to match the order of columns in the original dataset
    uploaded_data_sample = uploaded_data_sample[data.columns[:-1]]

    # Apply the same preprocessing steps as done for training data
    # Assuming the structure of uploaded data is similar to the original data
    uploaded_data_encoded = pd.DataFrame(encoder.transform(uploaded_data_sample), columns=uploaded_data_sample.columns)

    # Predict using the uploaded data
    for index, row in uploaded_data_encoded.iterrows():
        prediction, probability = predict(row)
        print(f"The Customer is eligible for AFFINITY_CARD (1/0)(Yes/No): {prediction}")
        print(f"Probability of being class 0: {probability[0]:.2f}")
        print(f"Probability of being class 1: {probability[1]:.2f}")


# Define function to handle file upload
def handle_file_upload(change):
    uploaded_filename = next(iter(file_upload.value))
    uploaded_file = file_upload.value[uploaded_filename]
    uploaded_data = pd.read_csv(io.BytesIO(uploaded_file['content']))
    # Predict using uploaded data
    predict_uploaded_data(uploaded_data)

def on_predict_button_clicked(b):
    input_data = [widget.value for widget in widgets_list]
    # Include CUST_ID automatically
    #input_data.insert(0, data['CUST_ID'].max() + 1)  # Assuming CUST_ID starts from 1 and increments by 1
    print(f"Input Data: {input_data}")
    prediction, probability = predict(input_data)
    print(f"The Customer is eligible for AFFINITY_CARD (1/0)(Yes/No): {prediction}")
    print(f"Probability of being class 0: {probability[0]:.2f}")
    print(f"Probability of being class 1: {probability[1]:.2f}")

# Display widgets and prediction button
for widget in widgets_list:
    display(widget)
predict_button = widgets.Button(description="Predict")
display(predict_button)

# Register the event handler
predict_button.on_click(on_predict_button_clicked)

# Display file upload widget
file_upload = widgets.FileUpload(accept='.csv', multiple=False)
file_upload.observe(handle_file_upload, names='value')
display(file_upload)

# Test the accuracy of the application
accuracy = model.score(X_test_encoded, y_test)
print(f"Accuracy of the application on the test data: {accuracy}")

"""# **Test Application based on logistic regression model using 100 random records from remaining datasets**"""

import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
import ipywidgets as widgets
import io

# List of columns used in the model
required_columns = ['AGE', 'CUST_GENDER', 'CUST_MARITAL_STATUS', 'CUST_INCOME_LEVEL', 'COUNTRY_NAME', 'EDUCATION', 'OCCUPATION', 'HOUSEHOLD_SIZE', 'YRS_RESIDENCE',
                    'BULK_PACK_DISKETTES', 'FLAT_PANEL_MONITOR', 'HOME_THEATER_PACKAGE', 'BOOKKEEPING_APPLICATION', 'Y_BOX_GAMES', 'OS_DOC_SET_KANJI']

# File upload widget
file_upload = widgets.FileUpload(accept='.csv', multiple=False)

# Function to handle file upload and process data
def handle_file_upload(change):
    # Get the uploaded file content
    uploaded_filename = list(file_upload.value.keys())[0]
    uploaded_file = file_upload.value[uploaded_filename]
    uploaded_data = pd.read_csv(io.BytesIO(uploaded_file['content']))

    # Select only the required columns
    # This ensures we're ignoring extra columns that aren't part of the model's input features
    uploaded_data_filtered = uploaded_data[required_columns]

    # Take a random sample of 100 customer records
    sample_data = uploaded_data_filtered.sample(n=100, random_state=42)

    # Apply encoding and make predictions
    sample_data_encoded = encoder.transform(sample_data)

    # Predict eligibility for affinity card
    for index, row in enumerate(sample_data_encoded):
        prediction, probability = predict(row)  # Assuming the predict function is defined
        print(f"Record {index + 1}: Eligible for Affinity Card: {prediction}")
        print(f"Probability of being class 0: {probability[0]:.2f}")
        print(f"Probability of being class 1: {probability[1]:.2f}")

# Attach the event handler to the file upload widget
file_upload.observe(handle_file_upload, names='value')

# Display the file upload widget
display(file_upload)